{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSTM\n",
    "\n",
    "Using Fake news classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1)- Import Key Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support both Python 2 and Python 3 with minimal overhead.\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "# I am an engineer. I care only about error not warning. So, let's be maverick and ignore warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hassan.sherwani\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Loading and preparing data\n",
    "\n",
    "- Dataset: https://www.kaggle.com/c/fake-news/data#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('train.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Drop Nan Values\n",
    "df=df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ready_data.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('ready_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the Independent Features\n",
    "\n",
    "X=df[['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the Dependent features\n",
    "y=df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18285, 1)\n",
      "(18285,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.Corpus building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0, len(titles)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', titles['title'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hous dem aid even see comey letter jason chaffetz tweet',\n",
       " 'flynn hillari clinton big woman campu breitbart',\n",
       " 'truth might get fire',\n",
       " 'civilian kill singl us airstrik identifi',\n",
       " 'iranian woman jail fiction unpublish stori woman stone death adulteri']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.One_hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4353, 2107, 3696, 190, 4014, 56, 2774, 1741, 2048, 1582],\n",
       " [1561, 1146, 1382, 2049, 4158, 3757, 3695],\n",
       " [3497, 1692, 755, 4558],\n",
       " [3965, 2120, 2303, 3894, 2523, 3275],\n",
       " [3484, 4158, 1283, 2527, 4078, 2157, 4158, 3397, 4874, 3230]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Vocabulary size\n",
    "voc_size=5000\n",
    "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
    "onehot_repr[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[hous dem aid even see comey letter jason chaffetz tweet] is encoded as\n",
    "\n",
    "[4353, 2107, 3696, 190, 4014, 56, 2774, 1741, 2048, 1582]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.Embedding Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0 4353 2107 3696  190\n",
      "  4014   56 2774 1741 2048 1582]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0 1561\n",
      "  1146 1382 2049 4158 3757 3695]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0 3497 1692  755 4558]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  3965 2120 2303 3894 2523 3275]\n",
      " [   0    0    0    0    0    0    0    0    0    0 3484 4158 1283 2527\n",
      "  4078 2157 4158 3397 4874 3230]]\n"
     ]
    }
   ],
   "source": [
    "#padding\n",
    "sent_length=20\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "print(embedded_docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that sentence lenth is 20 and our 1st sentence has 10 words. So all other words are padded as zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_features=40 # dimension of embedding layer\n",
    "voc_size=5000\n",
    "sent_length=20\n",
    "output_layer= 1 \n",
    "epochs=10\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Defining basic model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(LSTM(100)) # one lstm layer with 100 neurons\n",
    "model.add(Dense(output_layer,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 40)            200000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               56400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 256,501\n",
      "Trainable params: 256,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary params calculation\n",
    "\n",
    "https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are three steps:**\n",
    "\n",
    "- For embedding layer: Dim of embedded layer(40) * vocab_size(5000)\n",
    "\n",
    "- For LSTM Layer: Now formula is **#of params=g*[h(h+i)+biase]**\n",
    "\n",
    "where g=lstm layer length i.e 1. For FFNS, RNN has 1 , LSTM has 4 and GRU has 3 layers<br>\n",
    "h=hidden layer size (number of neurons in hidden layer) i.e 100<br> \n",
    "i=Input size/dimension i.e embedding_vector_features that is coming as an input on lstm layer in step2 (40 is value) <br>\n",
    "biase= 100 of neurons for outpt of lstm layer\n",
    "\n",
    "- For Dense (Output): lstm layer acting as input(100) * neurons in output layer(1) + biase(1 as there is only one neuron in output layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18285, (18285,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_docs),y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final=np.array(embedded_docs)\n",
    "y_final=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18285, 20)\n",
      "(18285,)\n"
     ]
    }
   ],
   "source": [
    "print(X_final.shape)\n",
    "print(y_final.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.split train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14628 samples, validate on 3657 samples\n",
      "Epoch 1/10\n",
      "14628/14628 [==============================] - 6s 416us/sample - loss: 0.3046 - accuracy: 0.8563 - val_loss: 0.2067 - val_accuracy: 0.9130\n",
      "Epoch 2/10\n",
      "14628/14628 [==============================] - 4s 241us/sample - loss: 0.1319 - accuracy: 0.9499 - val_loss: 0.2031 - val_accuracy: 0.9130\n",
      "Epoch 3/10\n",
      "14628/14628 [==============================] - 4s 247us/sample - loss: 0.0944 - accuracy: 0.9635 - val_loss: 0.2276 - val_accuracy: 0.9087\n",
      "Epoch 4/10\n",
      "14628/14628 [==============================] - 4s 248us/sample - loss: 0.0730 - accuracy: 0.9731 - val_loss: 0.2372 - val_accuracy: 0.9130\n",
      "Epoch 5/10\n",
      "14628/14628 [==============================] - 4s 245us/sample - loss: 0.0496 - accuracy: 0.9828 - val_loss: 0.3170 - val_accuracy: 0.9144\n",
      "Epoch 6/10\n",
      "14628/14628 [==============================] - 4s 243us/sample - loss: 0.0318 - accuracy: 0.9897 - val_loss: 0.3429 - val_accuracy: 0.9114\n",
      "Epoch 7/10\n",
      "14628/14628 [==============================] - 4s 248us/sample - loss: 0.0217 - accuracy: 0.9932 - val_loss: 0.4227 - val_accuracy: 0.9114\n",
      "Epoch 8/10\n",
      "14628/14628 [==============================] - 4s 253us/sample - loss: 0.0121 - accuracy: 0.9965 - val_loss: 0.4833 - val_accuracy: 0.9100\n",
      "Epoch 9/10\n",
      "14628/14628 [==============================] - 4s 255us/sample - loss: 0.0102 - accuracy: 0.9967 - val_loss: 0.5725 - val_accuracy: 0.9139\n",
      "Epoch 10/10\n",
      "14628/14628 [==============================] - 4s 254us/sample - loss: 0.0081 - accuracy: 0.9976 - val_loss: 0.5791 - val_accuracy: 0.9087\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x253819058c8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict_classes(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1918,  164],\n",
       "       [ 170, 1405]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9086683073557561"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.adding dropout\n",
    "\n",
    "results are sligtly improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 40)            200000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 20, 40)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               56400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 256,501\n",
      "Trainable params: 256,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "## Creating model\n",
    "embedding_vector_features=40\n",
    "model_dropout=Sequential()\n",
    "model_dropout.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model_dropout.add(Dropout(0.3))\n",
    "model_dropout.add(LSTM(100))\n",
    "model_dropout.add(Dropout(0.3))\n",
    "model_dropout.add(Dense(1,activation='sigmoid'))\n",
    "model_dropout.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "print(model_dropout.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14628 samples, validate on 3657 samples\n",
      "Epoch 1/10\n",
      "14628/14628 [==============================] - 6s 389us/sample - loss: 0.3190 - accuracy: 0.8512 - val_loss: 0.1968 - val_accuracy: 0.9128\n",
      "Epoch 2/10\n",
      "14628/14628 [==============================] - 4s 244us/sample - loss: 0.1430 - accuracy: 0.9441 - val_loss: 0.2135 - val_accuracy: 0.9117\n",
      "Epoch 3/10\n",
      "14628/14628 [==============================] - 4s 243us/sample - loss: 0.1065 - accuracy: 0.9610 - val_loss: 0.2161 - val_accuracy: 0.9144\n",
      "Epoch 4/10\n",
      "14628/14628 [==============================] - 3s 234us/sample - loss: 0.0758 - accuracy: 0.9727 - val_loss: 0.2304 - val_accuracy: 0.9144\n",
      "Epoch 5/10\n",
      "14628/14628 [==============================] - 3s 233us/sample - loss: 0.0572 - accuracy: 0.9809 - val_loss: 0.2914 - val_accuracy: 0.9193\n",
      "Epoch 6/10\n",
      "14628/14628 [==============================] - 4s 239us/sample - loss: 0.0416 - accuracy: 0.9864 - val_loss: 0.2960 - val_accuracy: 0.9144\n",
      "Epoch 7/10\n",
      "14628/14628 [==============================] - 3s 238us/sample - loss: 0.0312 - accuracy: 0.9897 - val_loss: 0.3324 - val_accuracy: 0.9158\n",
      "Epoch 8/10\n",
      "14628/14628 [==============================] - 3s 238us/sample - loss: 0.0229 - accuracy: 0.9926 - val_loss: 0.4188 - val_accuracy: 0.9155\n",
      "Epoch 9/10\n",
      "14628/14628 [==============================] - 3s 235us/sample - loss: 0.0201 - accuracy: 0.9934 - val_loss: 0.4144 - val_accuracy: 0.9141\n",
      "Epoch 10/10\n",
      "14628/14628 [==============================] - 3s 238us/sample - loss: 0.0172 - accuracy: 0.9949 - val_loss: 0.4786 - val_accuracy: 0.9141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2538c4b66c8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dropout.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model_dropout.predict_classes(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9141372709871479"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have got slightly better results by adding dropout. As our tarining accuracy slightly less on overfitting pattern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
